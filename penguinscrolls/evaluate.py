# penguinscrolls/evaluate.py
#
#
# This script evaluates the correctness of user answers generated by a language model against
# reference answers. It uses the OpenAI API to assess the answers based on a provided prompt
# and evaluation criteria.
#
#
# The script loads a dataset, generates responses from a file, evaluates each response,
# and saves the results to a JSON file.
#
#
# It supports parallel processing for improved efficiency.
#

from typing import List, Optional, Union

import fire
import pandas as pd
from jinja2 import Template
from openai import OpenAI
from pydantic import BaseModel, ConfigDict
from tqdm.auto import tqdm

from .defs import (
    ANSWER_COL,
    ERROR_COL,
    ERROR_PREFIX,
    EVAL_RESULT_COL,
    INPUT_COL,
    KEY_COL,
    PENGUIN_EVAL_MODEL,
    QUESTION_COL,
    RESPONSE_COL,
    SCORE_COL,
    TRUNCATED_COL,
)
from .util import get_mapper, get_penguin_dataset

openai = OpenAI()

template_no_evidence = Template(
    r"""Task Overview:
You are tasked with evaluating user answers based on a 
given question and reference answer. Your goal is to 
assess the correctness of the user answer using a 
specific metric.

Evaluation Criteria:
1. Yes/No Questions: Verify if the user's answer aligns 
with the reference answer in terms of a "yes" or "no" 
response.
2. Short Answers/Directives: Ensure key details such as 
numbers, specific nouns/verbs, and dates match those in 
the reference answer.
3. Abstractive/Long Answers: The user's answer can 
differ in wording but must convey the same meaning and 
contain the same key information as the reference 
answer to be considered correct.

Evaluation Process:
1. Identify the type of question presented.
2. Apply the relevant criteria from the Evaluation 
Criteria.
3. Compare the user's answer against the reference 
answer accordingly.
4. Score the answer with a binary label 0 or 1, where 0 
denotes wrong and 1 denotes correct.
NOTE that if the user answer is 0 or an empty string, it 
should get a 0 score.

Question: {{question}}
User Answer: {{sys_ans}}
Reference Answer: {{ref_ans}}

Evaluation Form (score ONLY):
- Correctness:"""
)


class EvalResult(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    score: Optional[Union[float, int]] = None  # 0到1之间的分数，便于取平均
    result: Optional[str] = None  # 原始结果


def ask(prompt: str) -> str:
    """
    Asks a question to the OpenAI API and returns the response.

    Args:
        prompt: The question to ask.

    Returns:
        The response from the OpenAI API, or an error message if something goes wrong.
    """
    try:
        resp = openai.chat.completions.create(
            model=PENGUIN_EVAL_MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=2048,
            top_p=0.9,
        )

        ret = resp.choices[0].message.content  # type: ignore
        if ret is None:
            return f'{ERROR_PREFIX}: "None" returned'
        return ret
    except Exception as ex:
        return f"{ERROR_PREFIX}: {str(ex)}"


class Row(BaseModel):
    input_md5: str
    input: str
    output: str
    prompt: str
    response: Optional[str] = None
    error: Optional[str] = None
    truncated: Optional[bool] = False


def eval_row(row: Row) -> EvalResult:
    if row.error is not None or row.truncated:
        return EvalResult(score=None, result=None)
    eval_prompt = template_no_evidence.render(
        question=row.prompt, sys_ans=row.response, ref_ans=row.output
    )
    eval_response = ask(eval_prompt).strip()
    if eval_response.startswith(ERROR_PREFIX):
        return EvalResult(score=None, result=eval_response)
    last_line = eval_response.split("\n")[-1]

    if "1" in last_line and "0" not in last_line:
        score = 1
    elif "0" in last_line and "1" not in last_line:
        score = 0
    else:
        score = None

    eval_result = EvalResult(result=eval_response, score=score)
    return eval_result


def main(
    input_filename: str,  # must have 'input_md5' and 'output' columns
    output_filename: str,
    concurrency: int = 1,
    limit: Optional[int] = None,
):
    dataset_df: pd.DataFrame = (
        get_penguin_dataset(limit=limit)
        .select_columns([INPUT_COL, KEY_COL, ANSWER_COL, QUESTION_COL])
        .to_pandas()
    )  # type: ignore
    response_df = pd.read_json(input_filename, lines=True)
    df = dataset_df.merge(response_df, on=KEY_COL, how="left")

    with get_mapper(concurrency) as mapper:
        rows = list(map(lambda x: Row.model_validate(x.to_dict()), df.iloc))
        eval_result: List[EvalResult] = list(
            tqdm(mapper(eval_row, rows), total=len(df))
        )
    result_df = pd.DataFrame(
        {
            KEY_COL: [i.input_md5 for i in rows],
            RESPONSE_COL: [i.response for i in rows],
            EVAL_RESULT_COL: [i.result for i in eval_result],
            SCORE_COL: [i.score for i in eval_result],
            ERROR_COL: [i.error for i in rows],
            TRUNCATED_COL: [i.truncated for i in rows],
        }
    )
    result_df.to_json(output_filename, lines=True, orient="records", force_ascii=False)


if __name__ == "__main__":
    fire.Fire(main)
